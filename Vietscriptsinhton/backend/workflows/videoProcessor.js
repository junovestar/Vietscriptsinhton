import { GeminiService } from '../services/geminiService.js'
import { DeepSeekService } from '../services/deepSeekService.js'
import { TranscriptService } from '../services/transcriptService.js'
import { ScriptGeneratorService } from '../services/scriptGeneratorService.js'

/**
 * Main workflow function that mirrors the n8n workflow
 * Processes YouTube video following exact same steps as n8n
 */
export async function processYouTubeVideo(youtubeUrl, settings, progressCallback = null) {
  console.log('ğŸ¬ Starting video processing workflow...')
  console.log('ğŸ“¹ URL:', youtubeUrl)
  
  const updateProgress = (step, segment = 0, total = 0, segmentInfo = null, estimatedTime = null) => {
    if (progressCallback) {
      progressCallback({
        currentStep: step,
        currentSegment: segment,
        totalSegments: total,
        currentSegmentInfo: segmentInfo,
        estimatedTime: estimatedTime
      })
    }
  }
  
  try {
    // Step 1: Set Transcript Prompt (equivalent to "Set: Transcript Prompt" node)
    const initialPrompt = "Determine the total duration of the video in [hh:mm:ss] format."
    
    console.log('â±ï¸ Step 1: Getting video duration...')
    updateProgress(1, 0, 0, null, 'Äang phÃ¢n tÃ­ch Ä‘á»™ dÃ i video...')
    
    // Step 2: HTTP Request to Google (equivalent to "HTTP Request to Google" node)
    const geminiService = new GeminiService(settings.apiKey)
    const durationResponse = await geminiService.analyzeVideo(youtubeUrl, initialPrompt)
    
    console.log('ğŸ“Š Duration response:', durationResponse)
    
    // Step 3: AI Agent + DeepSeek Chat Model (equivalent to "AI Agent" + "DeepSeek Chat Model" nodes)
    updateProgress(2, 0, 0, null, 'Äang dÃ¹ng DeepSeek chia video thÃ nh cÃ¡c Ä‘oáº¡n...')
    const deepSeekService = new DeepSeekService()
    const systemMessage = "Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  chia khoáº£ng thá»i gian nÃ y thÃ nh cÃ¡c item vá»›i giá»›i háº¡n lÃ  5 phÃºt, cá»© 5 phÃºt 1 item, tráº£ vá» káº¿t quáº£ lÃ  thá»i gian theo dáº¡ng hh:mm:ss."
    
    console.log('ğŸ¤– Step 2: Using DeepSeek to segment video into 5-minute chunks...')
    const timeSegments = await deepSeekService.processSegments(durationResponse, systemMessage)
    
    console.log('ğŸ“‹ Time segments:', timeSegments)
    
    // Calculate estimated time
    const estimatedMinutes = timeSegments.length * 1.5 + 5 // 1.5 min per segment + 5 min for final processing
    const estimatedTime = `Khoáº£ng ${Math.ceil(estimatedMinutes)} phÃºt`
    updateProgress(2, 0, timeSegments.length, null, estimatedTime)
    
    // Step 4: Split Out (equivalent to "Split Out" node)
    // Process each time segment
    const transcriptParts = []
    
    console.log('ğŸ”„ Step 3: Processing each segment...')
    updateProgress(3, 0, timeSegments.length, null, estimatedTime)
    
    for (let i = 0; i < timeSegments.length; i++) {
      const segment = timeSegments[i]
      console.log(`â° Processing segment ${i + 1}/${timeSegments.length}: ${segment.start} - ${segment.end}`)
      
      // Wait 1 minute between segments (equivalent to "Wait1" node)
      if (i > 0) {
        console.log('â³ Wait1: Waiting 1 minute before next segment...')
        await new Promise(resolve => setTimeout(resolve, 60000)) // 1 minute
      }
      
      // Update progress for current segment
      updateProgress(3, i + 1, timeSegments.length, segment, estimatedTime)
      
      // Step 5: Set Scene Clips (equivalent to "Set: Scene Clips" node)
      const scenePrompt = `Extract shareable clips for social media.  
Only analyze 5 minute from ${segment.start} to ${segment.end}.  
Do not skip any timeframe within this range. Break clips strictly by scene changes.  

Each clip must include:  
* Timestamp: [hh:mm:ss]-[hh:mm:ss]  
* Transcript: Verbatim dialogue/text within the clip.  
* Description: Describe the scene in chronological order. When the scene changes, start a new bullet point. Only describe events within the specified time range.  

How to describe:  
- Setting and environment: background, location, lighting, time of day, and other details.  
- Characters' clothing and expressions: outfits, facial expressions, body language, and emotions.  
- Actions and interactions: exact actions, interactions between characters, and camera movements such as panning, zooming, close-up, or wide shot.  

Start output directly with the response -- do not include any introductory text or explanations.`

      // Step 6: Wait (equivalent to "Wait1" node) - 1 minute delay
      console.log('â³ Waiting 1 minute to avoid rate limits...')
      await new Promise(resolve => setTimeout(resolve, 60000)) // 1 minute
      
      // Step 7: HTTP Request to Google1 (equivalent to "HTTP Request to Google1" node)
      // Using gemini-2.5-flash as specified in workflow
      const segmentTranscript = await geminiService.analyzeVideoSegment(youtubeUrl, scenePrompt, 'gemini-2.5-flash')
      
      // Step 8: Edit Fields (equivalent to "Edit Fields" node)
      transcriptParts.push(segmentTranscript)
      
      console.log(`âœ… Segment ${i + 1} processed successfully`)
    }
    
    // Step 9: Aggregate (equivalent to "Aggregate" node)
    console.log('ğŸ“ Step 4: Aggregating all transcripts...')
    updateProgress(4, 0, 0, null, 'Äang tá»•ng há»£p transcript...')
    const fullTranscript = transcriptParts.join('\n\n')
    
    // Step 10: Wait (equivalent to "Wait" node) - 3 minutes delay
    console.log('â³ Waiting 3 minutes before final processing...')
    await new Promise(resolve => setTimeout(resolve, 180000)) // 3 minutes
    
    // Step 11: Basic LLM Chain + Google Gemini Chat Model (equivalent to "Basic LLM Chain" + "Google Gemini Chat Model" nodes)
    console.log('ğŸ­ Step 5: Using Gemini 2.5 Pro to generate final script...')
    updateProgress(5, 0, 0, null, 'Äang táº¡o bÃ¬nh luáº­n cuá»‘i cÃ¹ng...')
    
    // Use the exact prompt from n8n workflow (Basic LLM Chain node)
    const workflowPrompt = `áº¡n lÃ  BiÃªn ká»‹ch viÃªn YouTube chuyÃªn nghiá»‡p, chuyÃªn viáº¿t ká»‹ch báº£n tÃ³m táº¯t video sinh tá»“n nÆ¡i hoang dÃ£.
Nhiá»‡m vá»¥: Viáº¿t láº¡i transcript thÃ nh ká»‹ch báº£n 10 phÃºt, Ä‘á»™ dÃ i 4500â€“5000 chá»¯, giá»¯ nguyÃªn ná»™i dung chÃ­nh, chá»‰ thÃªm mÃ´ táº£ ká»‹ch tÃ­nh vÃ  hÃ i hÆ°á»›c.
YÃªu cáº§u: 
- Ká»ƒ chuyá»‡n theo ngÃ´i thá»© 3.
- Chá»‰ xuáº¥t ra vÄƒn báº£n ká»‹ch báº£n, KHÃ”NG thÃªm kÃ½ tá»± Ä‘áº·c biá»‡t, KHÃ”NG markdown, KHÃ”NG format ngoÃ i chá»¯ thÆ°á»ng vÃ  chá»¯ hoa.
- Ná»™i dung pháº£i bÃ¡m sÃ¡t transcript, khÃ´ng thÃªm chi tiáº¿t bÃªn ngoÃ i.
- Káº¿t quáº£ pháº£i lÃ  chuá»—i text thuáº§n (plain text).
-Äáº§y Ä‘á»§ cÃ¡c hoáº¡t Ä‘á»™ng diá»…n ra trong script Ä‘Ã£ gá»­i, chá»‰ thÃªm mÃ´ táº£ chá»© khÃ´ng thÃªm gÃ¬ bÃªn ngoÃ i
- Náº¿u báº¡n láº¡c Ä‘á» khá»i dÃ n Ã½, hÃ£y tá»± sá»­a trÆ°á»›c khi gá»­i.



+YÃªu cáº§u ná»™i dung



-Phong cÃ¡ch ká»ƒ chuyá»‡n:




Ká»‹ch tÃ­nh, háº¥p dáº«n: Dáº«n dáº¯t ngÆ°á»i xem qua tá»«ng tÃ¬nh huá»‘ng sinh tá»“n cÄƒng tháº³ng, nhÆ°ng luÃ´n duy trÃ¬ sá»± láº¡c quan vÃ  hÃ i hÆ°á»›c.



-NhÃ¢n váº­t chÃ­nh:



Biá»‡t danh hÃ i hÆ°á»›c: Táº¡o biá»‡t danh cho nhÃ¢n váº­t chÃ­nh (vÃ­ dá»¥: "ThÃ¡nh Nhá» Rá»«ng SÃ¢u", "ChuyÃªn Gia Sinh Tá»“n Há»‡ 'tá»›i Ä‘áº§u hay tá»›i Ä‘Ã­t'").



-Bá»‘i cáº£nh vÃ  mÃ´i trÆ°á»ng:



MÃ´ táº£ sinh Ä‘á»™ng: MiÃªu táº£ cáº£nh váº­t xung quanh má»™t cÃ¡ch chi tiáº¿t, tá»« rá»«ng ráº­m, suá»‘i nÆ°á»›c, Ä‘áº¿n Ä‘á»™ng váº­t hoang dÃ£, nhÆ°ng luÃ´n lá»“ng ghÃ©p yáº¿u tá»‘ hÃ i hÆ°á»›c.



-Káº¿t thÃºc báº¥t ngá»:



Cao trÃ o ká»‹ch tÃ­nh: ÄÆ°a ngÆ°á»i xem Ä‘áº¿n Ä‘á»‰nh Ä‘iá»ƒm cÄƒng tháº³ng, nhÆ°ng káº¿t thÃºc láº¡i báº±ng má»™t tÃ¬nh huá»‘ng ngá»› ngáº©n hoáº·c hÃ i hÆ°á»›c, khiáº¿n ngÆ°á»i xem báº­t cÆ°á»i.   


HÃ£y tráº£ vá»  Ä‘Ãºng 1 plaintext duy nháº¥t`

    // Use Gemini 2.5 Pro model as specified in workflow
    const finalScript = await geminiService.generateScript(fullTranscript, workflowPrompt, 'gemini-2.5-pro')
    
    console.log('ğŸ‰ Workflow completed successfully!')
    return finalScript
    
  } catch (error) {
    console.error('âŒ Workflow error:', error)
    throw new Error(`Video processing failed: ${error.message}`)
  }
}

/**
 * Process uploaded video file
 */
export async function processVideoFile(filePath, settings) {
  console.log('ğŸ“ Processing uploaded video file...')
  
  try {
    // For uploaded files, we'll need to implement video analysis
    // This would require additional setup for local video processing
    throw new Error('File upload processing not yet implemented. Please use YouTube URLs for now.')
    
  } catch (error) {
    console.error('âŒ File processing error:', error)
    throw error
  }
}
